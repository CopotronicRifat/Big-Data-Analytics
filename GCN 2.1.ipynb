{"cells":[{"cell_type":"code","execution_count":1,"id":"2ed63e88","metadata":{"id":"2ed63e88","executionInfo":{"status":"ok","timestamp":1700282660832,"user_tz":360,"elapsed":3614,"user":{"displayName":"S M Rafiuddin Rifat","userId":"11981674521526715115"}}},"outputs":[],"source":["import numpy as np\n","import scipy.sparse as sp\n","import torch"]},{"cell_type":"code","execution_count":2,"id":"fa73fc8b","metadata":{"id":"fa73fc8b","executionInfo":{"status":"ok","timestamp":1700282660833,"user_tz":360,"elapsed":20,"user":{"displayName":"S M Rafiuddin Rifat","userId":"11981674521526715115"}}},"outputs":[],"source":["def encode_onehot(labels):\n","    classes = set(labels)\n","    classes_dict = {c: np.identity(len(classes))[i, :] for i, c in enumerate(classes)}\n","    labels_onehot = np.array(list(map(classes_dict.get, labels)), dtype=np.int32)\n","    return labels_onehot"]},{"cell_type":"code","execution_count":3,"id":"54da9116","metadata":{"id":"54da9116","executionInfo":{"status":"ok","timestamp":1700282660834,"user_tz":360,"elapsed":19,"user":{"displayName":"S M Rafiuddin Rifat","userId":"11981674521526715115"}}},"outputs":[],"source":["def feature_normalize(mx):\n","    \"\"\"Row-normalize sparse matrix\"\"\"\n","    rowsum = np.array(mx.sum(1))\n","    r_inv = np.power(rowsum, -1).flatten()\n","    r_inv[np.isinf(r_inv)] = 0.\n","    r_mat_inv = sp.diags(r_inv)\n","    mx = r_mat_inv.dot(mx)\n","    return mx"]},{"cell_type":"code","execution_count":4,"id":"4c3bc9ac","metadata":{"id":"4c3bc9ac","executionInfo":{"status":"ok","timestamp":1700282660834,"user_tz":360,"elapsed":17,"user":{"displayName":"S M Rafiuddin Rifat","userId":"11981674521526715115"}}},"outputs":[],"source":["def adj_normalize(mx):\n","    \"\"\"Row-normalize sparse matrix\"\"\"\n","    rowsum = np.array(mx.sum(1)) # Sum each row\n","    r_inv = np.power(rowsum, -1/2).flatten() # Negative square root\n","#     r_inv[np.isinf(r_inv)] = 0.\n","    r_mat_inv = sp.diags(r_inv) # Create diagonal matrix\n","\n","    # D^(-1/2).A.D^(-1/2)\n","    mx = r_mat_inv.dot(mx)\n","    mx = mx.dot(r_mat_inv)\n","    return mx"]},{"cell_type":"code","execution_count":5,"id":"73e762d1","metadata":{"id":"73e762d1","executionInfo":{"status":"ok","timestamp":1700282660835,"user_tz":360,"elapsed":17,"user":{"displayName":"S M Rafiuddin Rifat","userId":"11981674521526715115"}}},"outputs":[],"source":["def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n","    \"\"\"Convert a scipy sparse matrix to a torch sparse tensor.\"\"\"\n","    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n","    indices = torch.from_numpy(\n","        np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n","    values = torch.from_numpy(sparse_mx.data)\n","    shape = torch.Size(sparse_mx.shape)\n","    return torch.sparse.FloatTensor(indices, values, shape)"]},{"cell_type":"code","execution_count":6,"id":"ec661dc1","metadata":{"id":"ec661dc1","executionInfo":{"status":"ok","timestamp":1700282660835,"user_tz":360,"elapsed":16,"user":{"displayName":"S M Rafiuddin Rifat","userId":"11981674521526715115"}}},"outputs":[],"source":["def load_data(path=\"/content/data/cora/\", dataset=\"cora\"):\n","\n","    \"\"\"Load citation network dataset (cora only for now)\"\"\"\n","    print('Loading {} dataset...'.format(dataset))\n","\n","    idx_features_labels = np.genfromtxt(\"{}{}.content\".format(path, dataset),\n","                                        dtype=np.dtype(str))\n","    features = sp.csr_matrix(idx_features_labels[:, 1:-1], dtype=np.float32) # Processing features into a sparse matrix\n","    labels = encode_onehot(idx_features_labels[:, -1]) # one-hot encoding the labels\n","\n","\n","    # build graph\n","    idx = np.array(idx_features_labels[:, 0], dtype=np.int32) # Reading node-ids\n","    idx_map = {j: i for i, j in enumerate(idx)} # Creating index for nodes to map it in adjacency matrix\n","\n","    edges_unordered = np.genfromtxt(\"{}{}.cites\".format(path, dataset),\n","                                    dtype=np.int32) # Reading edges\n","    edges = np.array(list(map(idx_map.get, edges_unordered.flatten())),\n","                     dtype=np.int32).reshape(edges_unordered.shape) # Mapping node-ids in the edge list to the index\n","\n","    # Build adjacency matrix\n","    adj = sp.coo_matrix((np.ones(edges.shape[0]), (edges[:, 0], edges[:, 1])),\n","                        shape=(labels.shape[0], labels.shape[0]),\n","                        dtype=np.float32)\n","\n","    # CHECK OUT THE DIFFERENCES BETWEEN csr_matrix (features) and coo_matrix (adj)\n","\n","    # Normalizing features\n","    features = feature_normalize(features)\n","\n","#     # build symmetric adjacency matrix\n","#     adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n","\n","#     Normalizing the adjacency matrix after adding self loops\n","    adj = adj_normalize(adj + sp.eye(adj.shape[0]))\n","\n","    # Setting training, validation, and test range\n","    idx_train = range(140)\n","    idx_val = range(200, 500)\n","    idx_test = range(500, 1500)\n","\n","    # Converting all matrices into pytorch tensors\n","    features = torch.FloatTensor(np.array(features.todense()))\n","    labels = torch.LongTensor(np.where(labels)[1])\n","    adj = sparse_mx_to_torch_sparse_tensor(adj)\n","\n","    idx_train = torch.LongTensor(idx_train)\n","    idx_val = torch.LongTensor(idx_val)\n","    idx_test = torch.LongTensor(idx_test)\n","\n","    return adj, features, labels, idx_train, idx_val, idx_test"]},{"cell_type":"code","execution_count":7,"id":"5bce67b2","metadata":{"id":"5bce67b2","executionInfo":{"status":"ok","timestamp":1700282660835,"user_tz":360,"elapsed":15,"user":{"displayName":"S M Rafiuddin Rifat","userId":"11981674521526715115"}}},"outputs":[],"source":["# Function to find accuracy from two tensors\n","def accuracy(output, labels):\n","    preds = output.max(1)[1].type_as(labels) # Get the index of maximum value of 1 dimension and typecast to labels datatype\n","    correct = preds.eq(labels).double() # Convert into double\n","    correct = correct.sum() # Sum correct predictions\n","    return correct / len(labels)"]},{"cell_type":"code","execution_count":8,"id":"da7e8928","metadata":{"id":"da7e8928","executionInfo":{"status":"ok","timestamp":1700282660835,"user_tz":360,"elapsed":14,"user":{"displayName":"S M Rafiuddin Rifat","userId":"11981674521526715115"}}},"outputs":[],"source":["import math\n","\n","from torch.nn.parameter import Parameter\n","from torch.nn.modules.module import Module\n","\n","# Class to define a neural network layer that inherits PyTorch Module\n","# Check out documentaion of the base class 'Module' at:\n","# https://pytorch.org/docs/stable/generated/torch.nn.Module.html\n","class GraphConvolution(Module):\n","    # Each layer requires no. of input features, no. of output features, and optional bias\n","    def __init__(self, in_feat, out_feat, bias=True):\n","        super(GraphConvolution, self).__init__()\n","\n","        self.in_features = in_feat\n","        self.out_features = out_feat\n","\n","        # Using Parameter to automatically add weights and bias to learnable parameters\n","        #THIS WILL BE USEFUL ONLY WHEN WE USE Module in the model\n","        self.weight = Parameter(torch.FloatTensor(in_feat, out_feat))\n","\n","        if bias:\n","            self.bias = Parameter(torch.FloatTensor(out_feat))\n","        else:\n","            self.register_parameter('bias', None)\n","\n","        self.reset_parameters()\n","\n","    # Function to get uniform distribution of weights and bias values\n","    # Can be removed if necessary\n","    def reset_parameters(self):\n","        stdv = 1. / math.sqrt(self.weight.size(1))\n","        self.weight.data.uniform_(-stdv, stdv)\n","\n","        if self.bias is not None:\n","            self.bias.data.uniform_(-stdv, stdv)\n","\n","    # Forward function where it actually requires the input data and operations\n","    def forward(self, inp, adj):\n","        # Basically we multiply A.H,W\n","        support = torch.mm(inp, self.weight)\n","        output = torch.spmm(adj, support)\n","\n","        # Adding bias if true\n","        if self.bias is not None:\n","            return output + self.bias\n","        else:\n","            return output"]},{"cell_type":"code","execution_count":9,"id":"9b147458","metadata":{"id":"9b147458","executionInfo":{"status":"ok","timestamp":1700282660835,"user_tz":360,"elapsed":14,"user":{"displayName":"S M Rafiuddin Rifat","userId":"11981674521526715115"}}},"outputs":[],"source":["import torch.nn as nn\n","import torch.nn.functional as F\n","\n","# Class to define the model architecture\n","class GCN(nn.Module):\n","    # The model needs no. of input features, no. of hidden units,\n","    # no. of classes, and optional dropout\n","\n","    # NOTE: We use a simply model with one hidden layer\n","        # Architecture will change for deep models\n","        # Ideally, we keep only a few layers in most GNNs\n","    def __init__(self, nfeat, nhid, nclass, dropout):\n","        super(GCN, self).__init__()\n","\n","        # Defining one hidden layer and one output layer\n","        self.gcn1 = GraphConvolution(nfeat, nhid)\n","        # New second graph convolution layer\n","        self.gc2 = GraphConvolution(nhid, nhid)\n","        self.gcn2 = GraphConvolution(nhid, nclass)\n","        self.dropout = dropout\n","\n","    def forward(self, x, adj):\n","        # Use the correct attribute name for the first layer\n","        x = F.relu(self.gcn1(x, adj))\n","        x = F.dropout(x, self.dropout, training=self.training)\n","        # Use the correct attribute name for the second layer\n","        x = F.relu(self.gc2(x, adj))\n","        x = F.dropout(x, self.dropout, training=self.training)\n","        # Use the correct attribute name for the output layer\n","        x = self.gcn2(x, adj)\n","        return F.log_softmax(x, dim=1)"]},{"cell_type":"code","execution_count":10,"id":"990494b4","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"990494b4","executionInfo":{"status":"ok","timestamp":1700282666438,"user_tz":360,"elapsed":5614,"user":{"displayName":"S M Rafiuddin Rifat","userId":"11981674521526715115"}},"outputId":"25e7b7c9-836a-4496-e329-c057c99d4a54"},"outputs":[{"output_type":"stream","name":"stdout","text":["Loading cora dataset...\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-5-141d2dac508b>:8: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:605.)\n","  return torch.sparse.FloatTensor(indices, values, shape)\n"]}],"source":["import torch.optim as optim\n","\n","adj, features, labels, train_ids, val_ids, test_ids = load_data()\n","\n","# Model and optimizer\n","model = GCN(nfeat=features.shape[1],\n","            nhid=16,\n","            nclass=labels.max().item() + 1,\n","            dropout=0.5)\n","\n","# Using Adam optimizer. Other optimizer can be used too\n","optimizer = optim.Adam(model.parameters(),\n","                       lr=0.01, weight_decay=5e-4)"]},{"cell_type":"code","execution_count":11,"id":"e2542c4d","metadata":{"id":"e2542c4d","executionInfo":{"status":"ok","timestamp":1700282671691,"user_tz":360,"elapsed":5260,"user":{"displayName":"S M Rafiuddin Rifat","userId":"11981674521526715115"}}},"outputs":[],"source":["# Code for GPU computing\n","\n","# CHANGE THIS CODE TO SUIT YOUR VERSION OF PYTORCH. THE SYNTAX OF THIS COULD VARY SIGNIFICANTLY\n","\n","# If cuda is available, movie all data to gpu\n","# And preparing for CUDA operations\n","if torch.cuda.is_available():\n","    model.cuda()\n","    features = features.cuda()\n","    adj = adj.cuda()\n","    labels = labels.cuda()\n","    train_ids.cuda()\n","    val_ids.cuda()\n","    test_ids.cuda()"]},{"cell_type":"code","execution_count":12,"id":"146eb197","metadata":{"id":"146eb197","executionInfo":{"status":"ok","timestamp":1700282671692,"user_tz":360,"elapsed":33,"user":{"displayName":"S M Rafiuddin Rifat","userId":"11981674521526715115"}}},"outputs":[],"source":["import time\n","\n","def train(epoch):\n","    t = time.time()\n","\n","    # Evaluating the model in training mode\n","    model.train()\n","    optimizer.zero_grad() # Reseting gradient at each layer to avoid exploding gradient problem\n","    output = model(features, adj)\n","     # Optimizing with nll_loss. Other losses like cross_entropy can also be used\n","    loss_train = F.nll_loss(output[train_ids], labels[train_ids])\n","    acc_train = accuracy(output[train_ids], labels[train_ids])\n","    # backprop and optimize model parameters\n","    # Not needed to specify parameters when using Parameter\n","    loss_train.backward()\n","    optimizer.step()\n","\n","    # Evaluating validation performance separately\n","    model.eval()\n","    output = model(features, adj)\n","    loss_val = F.nll_loss(output[val_ids], labels[val_ids])\n","    acc_val = accuracy(output[val_ids], labels[val_ids])\n","\n","\n","    loss_val = F.nll_loss(output[val_ids], labels[val_ids])\n","    acc_val = accuracy(output[val_ids], labels[val_ids])\n","\n","    print('Epoch: {:04d}'.format(epoch+1),\n","          'loss_train: {:.4f}'.format(loss_train.item()),\n","          'acc_train: {:.4f}'.format(acc_train.item()),\n","          'loss_val: {:.4f}'.format(loss_val.item()),\n","          'acc_val: {:.4f}'.format(acc_val.item()),\n","          'time: {:.4f}s'.format(time.time() - t))"]},{"cell_type":"code","execution_count":13,"id":"b32be098","metadata":{"id":"b32be098","executionInfo":{"status":"ok","timestamp":1700282672401,"user_tz":360,"elapsed":739,"user":{"displayName":"S M Rafiuddin Rifat","userId":"11981674521526715115"}}},"outputs":[],"source":["from sklearn.metrics import precision_score, recall_score, f1_score\n","\n","def test():\n","    model.eval()\n","    output = model(features, adj)\n","    loss_test = F.nll_loss(output[test_ids], labels[test_ids])\n","    acc_test = accuracy(output[test_ids], labels[test_ids])\n","\n","    # Convert output probabilities to predicted class (highest probability)\n","    _, predicted = torch.max(output[test_ids], 1)\n","\n","    # Calculate Precision, Recall, and F1 Score\n","    precision = precision_score(labels[test_ids].cpu(), predicted.cpu(), average='macro')\n","    recall = recall_score(labels[test_ids].cpu(), predicted.cpu(), average='macro')\n","    f1 = f1_score(labels[test_ids].cpu(), predicted.cpu(), average='macro')\n","\n","    print(\"Test set results:\",\n","          \"loss= {:.4f}\".format(loss_test.item()),\n","          \"accuracy= {:.4f}\".format(acc_test.item()),\n","          \"precision= {:.4f}\".format(precision),\n","          \"recall= {:.4f}\".format(recall),\n","          \"F1= {:.4f}\".format(f1))\n"]},{"cell_type":"code","execution_count":14,"id":"a32da91b","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a32da91b","executionInfo":{"status":"ok","timestamp":1700282676780,"user_tz":360,"elapsed":4386,"user":{"displayName":"S M Rafiuddin Rifat","userId":"11981674521526715115"}},"outputId":"927eb2e0-b929-4e43-f738-4216fc376e29"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 0001 loss_train: 2.0098 acc_train: 0.1214 loss_val: 1.9845 acc_val: 0.0900 time: 2.6901s\n","Epoch: 0002 loss_train: 1.9974 acc_train: 0.0643 loss_val: 1.9625 acc_val: 0.0900 time: 0.0072s\n","Epoch: 0003 loss_train: 1.9764 acc_train: 0.1357 loss_val: 1.9419 acc_val: 0.1367 time: 0.0068s\n","Epoch: 0004 loss_train: 1.9521 acc_train: 0.1500 loss_val: 1.9222 acc_val: 0.2333 time: 0.0066s\n","Epoch: 0005 loss_train: 1.9172 acc_train: 0.2000 loss_val: 1.9037 acc_val: 0.3500 time: 0.0068s\n","Epoch: 0006 loss_train: 1.8897 acc_train: 0.2714 loss_val: 1.8873 acc_val: 0.3500 time: 0.0068s\n","Epoch: 0007 loss_train: 1.9179 acc_train: 0.2643 loss_val: 1.8747 acc_val: 0.3500 time: 0.0067s\n","Epoch: 0008 loss_train: 1.8695 acc_train: 0.2714 loss_val: 1.8656 acc_val: 0.3500 time: 0.0066s\n","Epoch: 0009 loss_train: 1.8751 acc_train: 0.2286 loss_val: 1.8603 acc_val: 0.3500 time: 0.0067s\n","Epoch: 0010 loss_train: 1.8466 acc_train: 0.2929 loss_val: 1.8576 acc_val: 0.3500 time: 0.0067s\n","Epoch: 0011 loss_train: 1.8251 acc_train: 0.2929 loss_val: 1.8572 acc_val: 0.3500 time: 0.0067s\n","Epoch: 0012 loss_train: 1.8338 acc_train: 0.2857 loss_val: 1.8578 acc_val: 0.3533 time: 0.0068s\n","Epoch: 0013 loss_train: 1.8375 acc_train: 0.2714 loss_val: 1.8583 acc_val: 0.3567 time: 0.0069s\n","Epoch: 0014 loss_train: 1.8045 acc_train: 0.3429 loss_val: 1.8572 acc_val: 0.3567 time: 0.0067s\n","Epoch: 0015 loss_train: 1.8416 acc_train: 0.2714 loss_val: 1.8543 acc_val: 0.3533 time: 0.0066s\n","Epoch: 0016 loss_train: 1.8163 acc_train: 0.2571 loss_val: 1.8503 acc_val: 0.3600 time: 0.0070s\n","Epoch: 0017 loss_train: 1.8064 acc_train: 0.3500 loss_val: 1.8451 acc_val: 0.3600 time: 0.0067s\n","Epoch: 0018 loss_train: 1.7833 acc_train: 0.3214 loss_val: 1.8394 acc_val: 0.3667 time: 0.0068s\n","Epoch: 0019 loss_train: 1.7850 acc_train: 0.3286 loss_val: 1.8330 acc_val: 0.3667 time: 0.0068s\n","Epoch: 0020 loss_train: 1.7778 acc_train: 0.3143 loss_val: 1.8266 acc_val: 0.3600 time: 0.0069s\n","Epoch: 0021 loss_train: 1.7307 acc_train: 0.3429 loss_val: 1.8196 acc_val: 0.3667 time: 0.0069s\n","Epoch: 0022 loss_train: 1.7354 acc_train: 0.4000 loss_val: 1.8123 acc_val: 0.3667 time: 0.0088s\n","Epoch: 0023 loss_train: 1.7358 acc_train: 0.3286 loss_val: 1.8048 acc_val: 0.3700 time: 0.0068s\n","Epoch: 0024 loss_train: 1.7545 acc_train: 0.3357 loss_val: 1.7968 acc_val: 0.3733 time: 0.0098s\n","Epoch: 0025 loss_train: 1.7061 acc_train: 0.4071 loss_val: 1.7883 acc_val: 0.3767 time: 0.0085s\n","Epoch: 0026 loss_train: 1.7215 acc_train: 0.3429 loss_val: 1.7794 acc_val: 0.3767 time: 0.0069s\n","Epoch: 0027 loss_train: 1.7221 acc_train: 0.3286 loss_val: 1.7700 acc_val: 0.3833 time: 0.0068s\n","Epoch: 0028 loss_train: 1.6633 acc_train: 0.3929 loss_val: 1.7604 acc_val: 0.3867 time: 0.0069s\n","Epoch: 0029 loss_train: 1.6756 acc_train: 0.3571 loss_val: 1.7503 acc_val: 0.3967 time: 0.0071s\n","Epoch: 0030 loss_train: 1.6445 acc_train: 0.3214 loss_val: 1.7393 acc_val: 0.3967 time: 0.0068s\n","Epoch: 0031 loss_train: 1.6265 acc_train: 0.4214 loss_val: 1.7277 acc_val: 0.3867 time: 0.0067s\n","Epoch: 0032 loss_train: 1.5935 acc_train: 0.3714 loss_val: 1.7148 acc_val: 0.3900 time: 0.0069s\n","Epoch: 0033 loss_train: 1.5908 acc_train: 0.3857 loss_val: 1.7010 acc_val: 0.3900 time: 0.0067s\n","Epoch: 0034 loss_train: 1.5818 acc_train: 0.3929 loss_val: 1.6865 acc_val: 0.3933 time: 0.0067s\n","Epoch: 0035 loss_train: 1.5666 acc_train: 0.4071 loss_val: 1.6696 acc_val: 0.3967 time: 0.0085s\n","Epoch: 0036 loss_train: 1.4747 acc_train: 0.4500 loss_val: 1.6522 acc_val: 0.3967 time: 0.0069s\n","Epoch: 0037 loss_train: 1.4765 acc_train: 0.4214 loss_val: 1.6348 acc_val: 0.4033 time: 0.0068s\n","Epoch: 0038 loss_train: 1.4730 acc_train: 0.4357 loss_val: 1.6179 acc_val: 0.4133 time: 0.0066s\n","Epoch: 0039 loss_train: 1.4359 acc_train: 0.4429 loss_val: 1.6010 acc_val: 0.4200 time: 0.0068s\n","Epoch: 0040 loss_train: 1.4154 acc_train: 0.4857 loss_val: 1.5839 acc_val: 0.4167 time: 0.0091s\n","Epoch: 0041 loss_train: 1.4362 acc_train: 0.4357 loss_val: 1.5671 acc_val: 0.4400 time: 0.0111s\n","Epoch: 0042 loss_train: 1.3759 acc_train: 0.4714 loss_val: 1.5505 acc_val: 0.4500 time: 0.0070s\n","Epoch: 0043 loss_train: 1.3039 acc_train: 0.5000 loss_val: 1.5344 acc_val: 0.4467 time: 0.0066s\n","Epoch: 0044 loss_train: 1.3474 acc_train: 0.4786 loss_val: 1.5197 acc_val: 0.4567 time: 0.0067s\n","Epoch: 0045 loss_train: 1.2709 acc_train: 0.5071 loss_val: 1.5053 acc_val: 0.4500 time: 0.0066s\n","Epoch: 0046 loss_train: 1.2601 acc_train: 0.5143 loss_val: 1.4904 acc_val: 0.4700 time: 0.0068s\n","Epoch: 0047 loss_train: 1.2183 acc_train: 0.4857 loss_val: 1.4775 acc_val: 0.4767 time: 0.0069s\n","Epoch: 0048 loss_train: 1.1953 acc_train: 0.5214 loss_val: 1.4637 acc_val: 0.4767 time: 0.0066s\n","Epoch: 0049 loss_train: 1.2282 acc_train: 0.5286 loss_val: 1.4508 acc_val: 0.4733 time: 0.0072s\n","Epoch: 0050 loss_train: 1.1554 acc_train: 0.5286 loss_val: 1.4392 acc_val: 0.4800 time: 0.0065s\n","Epoch: 0051 loss_train: 1.1213 acc_train: 0.5571 loss_val: 1.4281 acc_val: 0.4833 time: 0.0067s\n","Epoch: 0052 loss_train: 1.1426 acc_train: 0.5571 loss_val: 1.4177 acc_val: 0.4900 time: 0.0068s\n","Epoch: 0053 loss_train: 1.1388 acc_train: 0.5571 loss_val: 1.4084 acc_val: 0.4833 time: 0.0068s\n","Epoch: 0054 loss_train: 1.0290 acc_train: 0.5786 loss_val: 1.4006 acc_val: 0.4933 time: 0.0066s\n","Epoch: 0055 loss_train: 1.0797 acc_train: 0.5714 loss_val: 1.3926 acc_val: 0.4967 time: 0.0067s\n","Epoch: 0056 loss_train: 1.0585 acc_train: 0.5857 loss_val: 1.3849 acc_val: 0.5000 time: 0.0067s\n","Epoch: 0057 loss_train: 1.0135 acc_train: 0.5857 loss_val: 1.3789 acc_val: 0.4933 time: 0.0067s\n","Epoch: 0058 loss_train: 1.0216 acc_train: 0.5857 loss_val: 1.3740 acc_val: 0.4900 time: 0.0066s\n","Epoch: 0059 loss_train: 0.9995 acc_train: 0.5786 loss_val: 1.3673 acc_val: 0.4967 time: 0.0066s\n","Epoch: 0060 loss_train: 1.0310 acc_train: 0.5571 loss_val: 1.3564 acc_val: 0.5200 time: 0.0066s\n","Epoch: 0061 loss_train: 0.9893 acc_train: 0.6000 loss_val: 1.3509 acc_val: 0.5267 time: 0.0066s\n","Epoch: 0062 loss_train: 0.9518 acc_train: 0.6143 loss_val: 1.3463 acc_val: 0.5267 time: 0.0066s\n","Epoch: 0063 loss_train: 0.9576 acc_train: 0.6500 loss_val: 1.3411 acc_val: 0.5233 time: 0.0066s\n","Epoch: 0064 loss_train: 0.9063 acc_train: 0.6357 loss_val: 1.3392 acc_val: 0.5267 time: 0.0068s\n","Epoch: 0065 loss_train: 0.8817 acc_train: 0.6571 loss_val: 1.3398 acc_val: 0.5267 time: 0.0073s\n","Epoch: 0066 loss_train: 0.9121 acc_train: 0.6071 loss_val: 1.3448 acc_val: 0.5233 time: 0.0076s\n","Epoch: 0067 loss_train: 0.8673 acc_train: 0.6500 loss_val: 1.3466 acc_val: 0.5200 time: 0.0074s\n","Epoch: 0068 loss_train: 0.9147 acc_train: 0.6071 loss_val: 1.3455 acc_val: 0.5367 time: 0.0089s\n","Epoch: 0069 loss_train: 0.8784 acc_train: 0.6786 loss_val: 1.3461 acc_val: 0.5367 time: 0.0067s\n","Epoch: 0070 loss_train: 0.8437 acc_train: 0.6571 loss_val: 1.3441 acc_val: 0.5367 time: 0.0065s\n","Epoch: 0071 loss_train: 0.8579 acc_train: 0.6357 loss_val: 1.3329 acc_val: 0.5233 time: 0.0069s\n","Epoch: 0072 loss_train: 0.9179 acc_train: 0.6214 loss_val: 1.3233 acc_val: 0.5133 time: 0.0067s\n","Epoch: 0073 loss_train: 0.8103 acc_train: 0.6786 loss_val: 1.3224 acc_val: 0.5267 time: 0.0069s\n","Epoch: 0074 loss_train: 0.8026 acc_train: 0.6857 loss_val: 1.3218 acc_val: 0.5367 time: 0.0067s\n","Epoch: 0075 loss_train: 0.8280 acc_train: 0.6714 loss_val: 1.3197 acc_val: 0.5333 time: 0.0070s\n","Epoch: 0076 loss_train: 0.8022 acc_train: 0.7143 loss_val: 1.3212 acc_val: 0.5233 time: 0.0068s\n","Epoch: 0077 loss_train: 0.8130 acc_train: 0.6929 loss_val: 1.3289 acc_val: 0.5300 time: 0.0067s\n","Epoch: 0078 loss_train: 0.8018 acc_train: 0.7071 loss_val: 1.3398 acc_val: 0.5267 time: 0.0067s\n","Epoch: 0079 loss_train: 0.8086 acc_train: 0.6714 loss_val: 1.3321 acc_val: 0.5267 time: 0.0067s\n","Epoch: 0080 loss_train: 0.7528 acc_train: 0.7286 loss_val: 1.3237 acc_val: 0.5300 time: 0.0070s\n","Epoch: 0081 loss_train: 0.7324 acc_train: 0.6929 loss_val: 1.3221 acc_val: 0.5200 time: 0.0067s\n","Epoch: 0082 loss_train: 0.7488 acc_train: 0.6929 loss_val: 1.3258 acc_val: 0.5433 time: 0.0100s\n","Epoch: 0083 loss_train: 0.7012 acc_train: 0.7143 loss_val: 1.3331 acc_val: 0.5333 time: 0.0083s\n","Epoch: 0084 loss_train: 0.7246 acc_train: 0.7286 loss_val: 1.3405 acc_val: 0.5367 time: 0.0068s\n","Epoch: 0085 loss_train: 0.7814 acc_train: 0.6643 loss_val: 1.3438 acc_val: 0.5333 time: 0.0067s\n","Epoch: 0086 loss_train: 0.7907 acc_train: 0.6714 loss_val: 1.3413 acc_val: 0.5433 time: 0.0070s\n","Epoch: 0087 loss_train: 0.6505 acc_train: 0.7857 loss_val: 1.3334 acc_val: 0.5400 time: 0.0080s\n","Epoch: 0088 loss_train: 0.6796 acc_train: 0.7357 loss_val: 1.3211 acc_val: 0.5333 time: 0.0086s\n","Epoch: 0089 loss_train: 0.6824 acc_train: 0.7571 loss_val: 1.3165 acc_val: 0.5333 time: 0.0064s\n","Epoch: 0090 loss_train: 0.7092 acc_train: 0.7000 loss_val: 1.3155 acc_val: 0.5300 time: 0.0065s\n","Epoch: 0091 loss_train: 0.6476 acc_train: 0.7643 loss_val: 1.3164 acc_val: 0.5300 time: 0.0066s\n","Epoch: 0092 loss_train: 0.6728 acc_train: 0.7643 loss_val: 1.3147 acc_val: 0.5400 time: 0.0066s\n","Epoch: 0093 loss_train: 0.6895 acc_train: 0.7143 loss_val: 1.3167 acc_val: 0.5467 time: 0.0066s\n","Epoch: 0094 loss_train: 0.6838 acc_train: 0.7429 loss_val: 1.3179 acc_val: 0.5667 time: 0.0065s\n","Epoch: 0095 loss_train: 0.6524 acc_train: 0.7643 loss_val: 1.3216 acc_val: 0.5633 time: 0.0066s\n","Epoch: 0096 loss_train: 0.7077 acc_train: 0.7643 loss_val: 1.3207 acc_val: 0.5667 time: 0.0065s\n","Epoch: 0097 loss_train: 0.6550 acc_train: 0.7786 loss_val: 1.3209 acc_val: 0.5633 time: 0.0066s\n","Epoch: 0098 loss_train: 0.6574 acc_train: 0.7714 loss_val: 1.3213 acc_val: 0.5600 time: 0.0066s\n","Epoch: 0099 loss_train: 0.6328 acc_train: 0.7714 loss_val: 1.3196 acc_val: 0.5600 time: 0.0068s\n","Epoch: 0100 loss_train: 0.5922 acc_train: 0.7714 loss_val: 1.3174 acc_val: 0.5567 time: 0.0067s\n","Epoch: 0101 loss_train: 0.6966 acc_train: 0.7643 loss_val: 1.3153 acc_val: 0.5633 time: 0.0084s\n","Epoch: 0102 loss_train: 0.6073 acc_train: 0.8000 loss_val: 1.3158 acc_val: 0.5667 time: 0.0064s\n","Epoch: 0103 loss_train: 0.6339 acc_train: 0.7786 loss_val: 1.3140 acc_val: 0.5667 time: 0.0063s\n","Epoch: 0104 loss_train: 0.6388 acc_train: 0.7571 loss_val: 1.3162 acc_val: 0.5633 time: 0.0064s\n","Epoch: 0105 loss_train: 0.5633 acc_train: 0.8286 loss_val: 1.3207 acc_val: 0.5633 time: 0.0086s\n","Epoch: 0106 loss_train: 0.5922 acc_train: 0.7929 loss_val: 1.3213 acc_val: 0.5633 time: 0.0082s\n","Epoch: 0107 loss_train: 0.6102 acc_train: 0.7643 loss_val: 1.3289 acc_val: 0.5700 time: 0.0064s\n","Epoch: 0108 loss_train: 0.7175 acc_train: 0.7500 loss_val: 1.3260 acc_val: 0.5733 time: 0.0064s\n","Epoch: 0109 loss_train: 0.6481 acc_train: 0.7571 loss_val: 1.3211 acc_val: 0.5767 time: 0.0064s\n","Epoch: 0110 loss_train: 0.5969 acc_train: 0.7786 loss_val: 1.3161 acc_val: 0.5700 time: 0.0064s\n","Epoch: 0111 loss_train: 0.6280 acc_train: 0.7571 loss_val: 1.3116 acc_val: 0.5700 time: 0.0070s\n","Epoch: 0112 loss_train: 0.6299 acc_train: 0.7786 loss_val: 1.3120 acc_val: 0.5733 time: 0.0064s\n","Epoch: 0113 loss_train: 0.5866 acc_train: 0.7571 loss_val: 1.3098 acc_val: 0.5800 time: 0.0063s\n","Epoch: 0114 loss_train: 0.5861 acc_train: 0.7714 loss_val: 1.3128 acc_val: 0.5767 time: 0.0077s\n","Epoch: 0115 loss_train: 0.5882 acc_train: 0.7643 loss_val: 1.3200 acc_val: 0.5700 time: 0.0070s\n","Epoch: 0116 loss_train: 0.5759 acc_train: 0.7857 loss_val: 1.3291 acc_val: 0.5600 time: 0.0064s\n","Epoch: 0117 loss_train: 0.6161 acc_train: 0.7714 loss_val: 1.3406 acc_val: 0.5567 time: 0.0064s\n","Epoch: 0118 loss_train: 0.5688 acc_train: 0.7857 loss_val: 1.3368 acc_val: 0.5533 time: 0.0063s\n","Epoch: 0119 loss_train: 0.5477 acc_train: 0.8071 loss_val: 1.3317 acc_val: 0.5633 time: 0.0064s\n","Epoch: 0120 loss_train: 0.5937 acc_train: 0.7500 loss_val: 1.3282 acc_val: 0.5700 time: 0.0066s\n","Epoch: 0121 loss_train: 0.5322 acc_train: 0.8214 loss_val: 1.3244 acc_val: 0.5767 time: 0.0067s\n","Epoch: 0122 loss_train: 0.5802 acc_train: 0.7857 loss_val: 1.3243 acc_val: 0.5833 time: 0.0090s\n","Epoch: 0123 loss_train: 0.5757 acc_train: 0.7714 loss_val: 1.3271 acc_val: 0.5767 time: 0.0070s\n","Epoch: 0124 loss_train: 0.5867 acc_train: 0.7571 loss_val: 1.3319 acc_val: 0.5700 time: 0.0064s\n","Epoch: 0125 loss_train: 0.4713 acc_train: 0.8286 loss_val: 1.3371 acc_val: 0.5733 time: 0.0066s\n","Epoch: 0126 loss_train: 0.6350 acc_train: 0.7714 loss_val: 1.3398 acc_val: 0.5800 time: 0.0065s\n","Epoch: 0127 loss_train: 0.5220 acc_train: 0.8000 loss_val: 1.3492 acc_val: 0.5733 time: 0.0065s\n","Epoch: 0128 loss_train: 0.5944 acc_train: 0.7786 loss_val: 1.3563 acc_val: 0.5767 time: 0.0065s\n","Epoch: 0129 loss_train: 0.5695 acc_train: 0.7929 loss_val: 1.3514 acc_val: 0.5733 time: 0.0067s\n","Epoch: 0130 loss_train: 0.5534 acc_train: 0.7786 loss_val: 1.3496 acc_val: 0.5767 time: 0.0064s\n","Epoch: 0131 loss_train: 0.5457 acc_train: 0.7714 loss_val: 1.3466 acc_val: 0.5767 time: 0.0069s\n","Epoch: 0132 loss_train: 0.5873 acc_train: 0.8071 loss_val: 1.3410 acc_val: 0.5733 time: 0.0077s\n","Epoch: 0133 loss_train: 0.5861 acc_train: 0.7571 loss_val: 1.3465 acc_val: 0.5733 time: 0.0066s\n","Epoch: 0134 loss_train: 0.5692 acc_train: 0.7786 loss_val: 1.3596 acc_val: 0.5667 time: 0.0065s\n","Epoch: 0135 loss_train: 0.5862 acc_train: 0.7714 loss_val: 1.3820 acc_val: 0.5700 time: 0.0072s\n","Epoch: 0136 loss_train: 0.5842 acc_train: 0.7571 loss_val: 1.3902 acc_val: 0.5667 time: 0.0071s\n","Epoch: 0137 loss_train: 0.4999 acc_train: 0.8000 loss_val: 1.3856 acc_val: 0.5667 time: 0.0098s\n","Epoch: 0138 loss_train: 0.5646 acc_train: 0.7857 loss_val: 1.3630 acc_val: 0.5700 time: 0.0071s\n","Epoch: 0139 loss_train: 0.5449 acc_train: 0.7929 loss_val: 1.3483 acc_val: 0.5767 time: 0.0084s\n","Epoch: 0140 loss_train: 0.5548 acc_train: 0.7643 loss_val: 1.3461 acc_val: 0.5667 time: 0.0067s\n","Epoch: 0141 loss_train: 0.5561 acc_train: 0.8071 loss_val: 1.3474 acc_val: 0.5633 time: 0.0068s\n","Epoch: 0142 loss_train: 0.5445 acc_train: 0.7857 loss_val: 1.3479 acc_val: 0.5667 time: 0.0070s\n","Epoch: 0143 loss_train: 0.5325 acc_train: 0.7714 loss_val: 1.3505 acc_val: 0.5667 time: 0.0068s\n","Epoch: 0144 loss_train: 0.4793 acc_train: 0.8357 loss_val: 1.3529 acc_val: 0.5667 time: 0.0072s\n","Epoch: 0145 loss_train: 0.5204 acc_train: 0.8000 loss_val: 1.3593 acc_val: 0.5833 time: 0.0067s\n","Epoch: 0146 loss_train: 0.4636 acc_train: 0.8071 loss_val: 1.3740 acc_val: 0.5933 time: 0.0071s\n","Epoch: 0147 loss_train: 0.5915 acc_train: 0.7500 loss_val: 1.3820 acc_val: 0.5833 time: 0.0068s\n","Epoch: 0148 loss_train: 0.5354 acc_train: 0.7571 loss_val: 1.3853 acc_val: 0.5767 time: 0.0066s\n","Epoch: 0149 loss_train: 0.4965 acc_train: 0.7857 loss_val: 1.3792 acc_val: 0.5833 time: 0.0067s\n","Epoch: 0150 loss_train: 0.5416 acc_train: 0.7786 loss_val: 1.3687 acc_val: 0.5833 time: 0.0068s\n","Epoch: 0151 loss_train: 0.5074 acc_train: 0.7857 loss_val: 1.3639 acc_val: 0.5867 time: 0.0067s\n","Epoch: 0152 loss_train: 0.4680 acc_train: 0.7857 loss_val: 1.3635 acc_val: 0.5833 time: 0.0066s\n","Epoch: 0153 loss_train: 0.5564 acc_train: 0.7929 loss_val: 1.3621 acc_val: 0.5867 time: 0.0069s\n","Epoch: 0154 loss_train: 0.5025 acc_train: 0.8143 loss_val: 1.3625 acc_val: 0.5833 time: 0.0069s\n","Epoch: 0155 loss_train: 0.4978 acc_train: 0.8214 loss_val: 1.3661 acc_val: 0.5800 time: 0.0065s\n","Epoch: 0156 loss_train: 0.5568 acc_train: 0.7929 loss_val: 1.3741 acc_val: 0.5767 time: 0.0066s\n","Epoch: 0157 loss_train: 0.6326 acc_train: 0.7286 loss_val: 1.3768 acc_val: 0.5667 time: 0.0076s\n","Epoch: 0158 loss_train: 0.4672 acc_train: 0.8429 loss_val: 1.3809 acc_val: 0.5667 time: 0.0065s\n","Epoch: 0159 loss_train: 0.4496 acc_train: 0.8071 loss_val: 1.3869 acc_val: 0.5700 time: 0.0064s\n","Epoch: 0160 loss_train: 0.5427 acc_train: 0.8000 loss_val: 1.3931 acc_val: 0.5700 time: 0.0100s\n","Epoch: 0161 loss_train: 0.4970 acc_train: 0.7929 loss_val: 1.3950 acc_val: 0.5733 time: 0.0065s\n","Epoch: 0162 loss_train: 0.4869 acc_train: 0.8071 loss_val: 1.3944 acc_val: 0.5633 time: 0.0066s\n","Epoch: 0163 loss_train: 0.5143 acc_train: 0.7857 loss_val: 1.3950 acc_val: 0.5667 time: 0.0063s\n","Epoch: 0164 loss_train: 0.4950 acc_train: 0.8071 loss_val: 1.3968 acc_val: 0.5633 time: 0.0067s\n","Epoch: 0165 loss_train: 0.5549 acc_train: 0.7786 loss_val: 1.4006 acc_val: 0.5633 time: 0.0066s\n","Epoch: 0166 loss_train: 0.5418 acc_train: 0.7929 loss_val: 1.4052 acc_val: 0.5633 time: 0.0066s\n","Epoch: 0167 loss_train: 0.4869 acc_train: 0.8143 loss_val: 1.4064 acc_val: 0.5600 time: 0.0066s\n","Epoch: 0168 loss_train: 0.4624 acc_train: 0.8429 loss_val: 1.4114 acc_val: 0.5600 time: 0.0079s\n","Epoch: 0169 loss_train: 0.4842 acc_train: 0.8357 loss_val: 1.4144 acc_val: 0.5633 time: 0.0064s\n","Epoch: 0170 loss_train: 0.5219 acc_train: 0.7786 loss_val: 1.4132 acc_val: 0.5667 time: 0.0064s\n","Epoch: 0171 loss_train: 0.4993 acc_train: 0.8500 loss_val: 1.4172 acc_val: 0.5700 time: 0.0065s\n","Epoch: 0172 loss_train: 0.5317 acc_train: 0.8286 loss_val: 1.4158 acc_val: 0.5667 time: 0.0067s\n","Epoch: 0173 loss_train: 0.4983 acc_train: 0.8357 loss_val: 1.4130 acc_val: 0.5600 time: 0.0076s\n","Epoch: 0174 loss_train: 0.4688 acc_train: 0.8286 loss_val: 1.4172 acc_val: 0.5733 time: 0.0066s\n","Epoch: 0175 loss_train: 0.5592 acc_train: 0.7857 loss_val: 1.4272 acc_val: 0.5667 time: 0.0073s\n","Epoch: 0176 loss_train: 0.4565 acc_train: 0.8500 loss_val: 1.4367 acc_val: 0.5700 time: 0.0065s\n","Epoch: 0177 loss_train: 0.5234 acc_train: 0.8000 loss_val: 1.4361 acc_val: 0.5733 time: 0.0066s\n","Epoch: 0178 loss_train: 0.4939 acc_train: 0.8286 loss_val: 1.4312 acc_val: 0.5700 time: 0.0065s\n","Epoch: 0179 loss_train: 0.4616 acc_train: 0.8357 loss_val: 1.4237 acc_val: 0.5767 time: 0.0082s\n","Epoch: 0180 loss_train: 0.4716 acc_train: 0.8214 loss_val: 1.4244 acc_val: 0.5733 time: 0.0065s\n","Epoch: 0181 loss_train: 0.4365 acc_train: 0.8286 loss_val: 1.4326 acc_val: 0.5733 time: 0.0065s\n","Epoch: 0182 loss_train: 0.4661 acc_train: 0.8143 loss_val: 1.4370 acc_val: 0.5700 time: 0.0065s\n","Epoch: 0183 loss_train: 0.4111 acc_train: 0.8429 loss_val: 1.4340 acc_val: 0.5767 time: 0.0105s\n","Epoch: 0184 loss_train: 0.4316 acc_train: 0.8714 loss_val: 1.4296 acc_val: 0.5833 time: 0.0065s\n","Epoch: 0185 loss_train: 0.4695 acc_train: 0.8357 loss_val: 1.4322 acc_val: 0.5833 time: 0.0064s\n","Epoch: 0186 loss_train: 0.3950 acc_train: 0.8857 loss_val: 1.4350 acc_val: 0.5800 time: 0.0063s\n","Epoch: 0187 loss_train: 0.4327 acc_train: 0.8357 loss_val: 1.4375 acc_val: 0.5833 time: 0.0064s\n","Epoch: 0188 loss_train: 0.4998 acc_train: 0.8429 loss_val: 1.4447 acc_val: 0.5867 time: 0.0089s\n","Epoch: 0189 loss_train: 0.4918 acc_train: 0.8500 loss_val: 1.4522 acc_val: 0.5900 time: 0.0064s\n","Epoch: 0190 loss_train: 0.4337 acc_train: 0.8357 loss_val: 1.4680 acc_val: 0.5867 time: 0.0065s\n","Epoch: 0191 loss_train: 0.4498 acc_train: 0.8000 loss_val: 1.4767 acc_val: 0.5833 time: 0.0082s\n","Epoch: 0192 loss_train: 0.4122 acc_train: 0.8429 loss_val: 1.4830 acc_val: 0.5833 time: 0.0064s\n","Epoch: 0193 loss_train: 0.4770 acc_train: 0.8714 loss_val: 1.4782 acc_val: 0.5833 time: 0.0063s\n","Epoch: 0194 loss_train: 0.5000 acc_train: 0.8286 loss_val: 1.4731 acc_val: 0.5767 time: 0.0066s\n","Epoch: 0195 loss_train: 0.4785 acc_train: 0.8571 loss_val: 1.4712 acc_val: 0.5700 time: 0.0107s\n","Epoch: 0196 loss_train: 0.5282 acc_train: 0.7929 loss_val: 1.4775 acc_val: 0.5667 time: 0.0092s\n","Epoch: 0197 loss_train: 0.5305 acc_train: 0.8071 loss_val: 1.4841 acc_val: 0.5667 time: 0.0070s\n","Epoch: 0198 loss_train: 0.5005 acc_train: 0.8429 loss_val: 1.4801 acc_val: 0.5700 time: 0.0066s\n","Epoch: 0199 loss_train: 0.4509 acc_train: 0.8286 loss_val: 1.4621 acc_val: 0.5700 time: 0.0064s\n","Epoch: 0200 loss_train: 0.4648 acc_train: 0.8286 loss_val: 1.4574 acc_val: 0.5700 time: 0.0064s\n","\n","Optimization finished...\n","Total time elapsed: 4.4958s\n","Test set results: loss= 1.4872 accuracy= 0.5180 precision= 0.4637 recall= 0.4889 F1= 0.4698\n"]}],"source":["t_total = time.time()\n","\n","for epoch in range(200):\n","    train(epoch)\n","\n","print()\n","print('Optimization finished...')\n","print('Total time elapsed: {:.4f}s'.format(time.time() - t_total))\n","\n","test()"]},{"cell_type":"markdown","id":"1af85139","metadata":{"id":"1af85139"},"source":["###"]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"},"colab":{"provenance":[{"file_id":"1Yx-MX71fCfPeEh3DaoHG-ejh2x-dpA_Q","timestamp":1700282436610}],"gpuType":"T4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":5}