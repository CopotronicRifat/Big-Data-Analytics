{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 155,
      "id": "2ed63e88",
      "metadata": {
        "id": "2ed63e88"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import scipy.sparse as sp\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 156,
      "id": "fa73fc8b",
      "metadata": {
        "id": "fa73fc8b"
      },
      "outputs": [],
      "source": [
        "def encode_onehot(labels):\n",
        "    classes = set(labels)\n",
        "    classes_dict = {c: np.identity(len(classes))[i, :] for i, c in enumerate(classes)}\n",
        "    labels_onehot = np.array(list(map(classes_dict.get, labels)), dtype=np.int32)\n",
        "    return labels_onehot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 157,
      "id": "54da9116",
      "metadata": {
        "id": "54da9116"
      },
      "outputs": [],
      "source": [
        "def feature_normalize(mx):\n",
        "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
        "    rowsum = np.array(mx.sum(1))\n",
        "    r_inv = np.power(rowsum, -1).flatten()\n",
        "    r_inv[np.isinf(r_inv)] = 0.\n",
        "    r_mat_inv = sp.diags(r_inv)\n",
        "    mx = r_mat_inv.dot(mx)\n",
        "    return mx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 158,
      "id": "4c3bc9ac",
      "metadata": {
        "id": "4c3bc9ac"
      },
      "outputs": [],
      "source": [
        "def adj_normalize(mx):\n",
        "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
        "    rowsum = np.array(mx.sum(1)) # Sum each row\n",
        "    r_inv = np.power(rowsum, -1/2).flatten() # Negative square root\n",
        "#     r_inv[np.isinf(r_inv)] = 0.\n",
        "    r_mat_inv = sp.diags(r_inv) # Create diagonal matrix\n",
        "\n",
        "    # D^(-1/2).A.D^(-1/2)\n",
        "    mx = r_mat_inv.dot(mx)\n",
        "    mx = mx.dot(r_mat_inv)\n",
        "    return mx"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def limit_neighbors(adj, max_neighbors):\n",
        "    modified_adj = np.copy(adj)\n",
        "    for node in range(adj.shape[0]):\n",
        "        neighbors = np.nonzero(adj[node])[0]\n",
        "        if len(neighbors) > max_neighbors:\n",
        "            np.random.shuffle(neighbors)\n",
        "            neighbors_to_remove = neighbors[max_neighbors:]\n",
        "            modified_adj[node, neighbors_to_remove] = 0\n",
        "            modified_adj[neighbors_to_remove, node] = 0\n",
        "    return modified_adj"
      ],
      "metadata": {
        "id": "-4Lb6woc6kgg"
      },
      "id": "-4Lb6woc6kgg",
      "execution_count": 159,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 160,
      "id": "73e762d1",
      "metadata": {
        "id": "73e762d1"
      },
      "outputs": [],
      "source": [
        "def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
        "    \"\"\"Convert a scipy sparse matrix to a torch sparse tensor.\"\"\"\n",
        "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
        "    indices = torch.from_numpy(\n",
        "        np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
        "    values = torch.from_numpy(sparse_mx.data)\n",
        "    shape = torch.Size(sparse_mx.shape)\n",
        "    return torch.sparse.FloatTensor(indices, values, shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 161,
      "id": "ec661dc1",
      "metadata": {
        "id": "ec661dc1"
      },
      "outputs": [],
      "source": [
        "def load_data(path=\"/content/data/cora/\", dataset=\"cora\"):\n",
        "\n",
        "    \"\"\"Load citation network dataset (cora only for now)\"\"\"\n",
        "    print('Loading {} dataset...'.format(dataset))\n",
        "\n",
        "    idx_features_labels = np.genfromtxt(\"{}{}.content\".format(path, dataset),\n",
        "                                        dtype=np.dtype(str))\n",
        "    features = sp.csr_matrix(idx_features_labels[:, 1:-1], dtype=np.float32) # Processing features into a sparse matrix\n",
        "    labels = encode_onehot(idx_features_labels[:, -1]) # one-hot encoding the labels\n",
        "\n",
        "\n",
        "    # build graph\n",
        "    idx = np.array(idx_features_labels[:, 0], dtype=np.int32) # Reading node-ids\n",
        "    idx_map = {j: i for i, j in enumerate(idx)} # Creating index for nodes to map it in adjacency matrix\n",
        "\n",
        "    edges_unordered = np.genfromtxt(\"{}{}.cites\".format(path, dataset),\n",
        "                                    dtype=np.int32) # Reading edges\n",
        "    edges = np.array(list(map(idx_map.get, edges_unordered.flatten())),\n",
        "                     dtype=np.int32).reshape(edges_unordered.shape) # Mapping node-ids in the edge list to the index\n",
        "\n",
        "    # Build adjacency matrix\n",
        "    adj = sp.coo_matrix((np.ones(edges.shape[0]), (edges[:, 0], edges[:, 1])),\n",
        "                        shape=(labels.shape[0], labels.shape[0]),\n",
        "                        dtype=np.float32)\n",
        "\n",
        "    # CHECK OUT THE DIFFERENCES BETWEEN csr_matrix (features) and coo_matrix (adj)\n",
        "\n",
        "    # Normalizing features\n",
        "    features = feature_normalize(features)\n",
        "\n",
        "#     # build symmetric adjacency matrix\n",
        "#     adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
        "\n",
        "#     Normalizing the adjacency matrix after adding self loops\n",
        "    adj = adj_normalize(adj + sp.eye(adj.shape[0]))\n",
        "\n",
        "    # Setting training, validation, and test range\n",
        "    idx_train = range(140)\n",
        "    idx_val = range(200, 500)\n",
        "    idx_test = range(500, 1500)\n",
        "\n",
        "    # Converting all matrices into pytorch tensors\n",
        "    features = torch.FloatTensor(np.array(features.todense()))\n",
        "    labels = torch.LongTensor(np.where(labels)[1])\n",
        "    adj = sparse_mx_to_torch_sparse_tensor(adj)\n",
        "\n",
        "    idx_train = torch.LongTensor(idx_train)\n",
        "    idx_val = torch.LongTensor(idx_val)\n",
        "    idx_test = torch.LongTensor(idx_test)\n",
        "\n",
        "    return adj, features, labels, idx_train, idx_val, idx_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 162,
      "id": "5bce67b2",
      "metadata": {
        "id": "5bce67b2"
      },
      "outputs": [],
      "source": [
        "# Function to find accuracy from two tensors\n",
        "def accuracy(output, labels):\n",
        "    preds = output.max(1)[1].type_as(labels) # Get the index of maximum value of 1 dimension and typecast to labels datatype\n",
        "    correct = preds.eq(labels).double() # Convert into double\n",
        "    correct = correct.sum() # Sum correct predictions\n",
        "    return correct / len(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 163,
      "id": "da7e8928",
      "metadata": {
        "id": "da7e8928"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "from torch.nn.parameter import Parameter\n",
        "from torch.nn.modules.module import Module\n",
        "\n",
        "# Class to define a neural network layer that inherits PyTorch Module\n",
        "# Check out documentaion of the base class 'Module' at:\n",
        "# https://pytorch.org/docs/stable/generated/torch.nn.Module.html\n",
        "class GraphConvolution(Module):\n",
        "    # Each layer requires no. of input features, no. of output features, and optional bias\n",
        "    def __init__(self, in_feat, out_feat, bias=True):\n",
        "        super(GraphConvolution, self).__init__()\n",
        "\n",
        "        self.in_features = in_feat\n",
        "        self.out_features = out_feat\n",
        "\n",
        "        # Using Parameter to automatically add weights and bias to learnable parameters\n",
        "        #THIS WILL BE USEFUL ONLY WHEN WE USE Module in the model\n",
        "        self.weight = Parameter(torch.FloatTensor(in_feat, out_feat))\n",
        "\n",
        "        if bias:\n",
        "            self.bias = Parameter(torch.FloatTensor(out_feat))\n",
        "        else:\n",
        "            self.register_parameter('bias', None)\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "    # Function to get uniform distribution of weights and bias values\n",
        "    # Can be removed if necessary\n",
        "    def reset_parameters(self):\n",
        "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
        "        self.weight.data.uniform_(-stdv, stdv)\n",
        "\n",
        "        if self.bias is not None:\n",
        "            self.bias.data.uniform_(-stdv, stdv)\n",
        "\n",
        "    # Forward function where it actually requires the input data and operations\n",
        "    def forward(self, inp, adj):\n",
        "        # Basically we multiply A.H,W\n",
        "        support = torch.mm(inp, self.weight)\n",
        "        output = torch.spmm(adj, support)\n",
        "\n",
        "        # Adding bias if true\n",
        "        if self.bias is not None:\n",
        "            return output + self.bias\n",
        "        else:\n",
        "            return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 164,
      "id": "9b147458",
      "metadata": {
        "id": "9b147458"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Class to define the model architecture\n",
        "class GCN(nn.Module):\n",
        "    # The model needs no. of input features, no. of hidden units,\n",
        "    # no. of classes, and optional dropout\n",
        "\n",
        "    # NOTE: We use a simply model with one hidden layer\n",
        "        # Architecture will change for deep models\n",
        "        # Ideally, we keep only a few layers in most GNNs\n",
        "    def __init__(self, nfeat, nhid, nclass, dropout):\n",
        "        super(GCN, self).__init__()\n",
        "\n",
        "        # Defining one hidden layer and one output layer\n",
        "        self.gcn1 = GraphConvolution(nfeat, nhid)\n",
        "        self.gcn2 = GraphConvolution(nhid, nclass)\n",
        "        self.dropout = dropout\n",
        "\n",
        "    # Similar to GraphConvolution, we give required input data to the forward function\n",
        "    # And specify operations - here it is activation and dropout\n",
        "    def forward(self, x, adj):\n",
        "        x = F.relu(self.gcn1(x, adj)) # Applying non-linearity on hidden layer 1\n",
        "        # Checkout difference between nn.Dropout() and F.dropout()\n",
        "        x = F.dropout(x, self.dropout, training=self.training)\n",
        "        x = self.gcn2(x, adj)\n",
        "        return F.log_softmax(x, dim=1) # Applying lograthmic softmax on output layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 165,
      "id": "990494b4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "990494b4",
        "outputId": "dc18bfb7-2db7-4cad-976c-b33540819a9d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading cora dataset...\n"
          ]
        }
      ],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# Loading data\n",
        "adj, features, labels, idx_train, idx_val, idx_test = load_data()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Model and optimizer\n",
        "model = GCN(nfeat=features.shape[1],\n",
        "            nhid=16,\n",
        "            nclass=labels.max().item() + 1,\n",
        "            dropout=0.5)\n",
        "\n",
        "\n",
        "# Using Adam optimizer. Other optimizer can be used too\n",
        "optimizer = optim.Adam(model.parameters(),\n",
        "                       lr=0.01, weight_decay=5e-4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 166,
      "id": "e2542c4d",
      "metadata": {
        "id": "e2542c4d"
      },
      "outputs": [],
      "source": [
        "# Code for GPU computing\n",
        "\n",
        "# CHANGE THIS CODE TO SUIT YOUR VERSION OF PYTORCH. THE SYNTAX OF THIS COULD VARY SIGNIFICANTLY\n",
        "\n",
        "# If cuda is available, movie all data to gpu\n",
        "\n",
        "# Set device to CUDA if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Move model and tensors to the chosen device\n",
        "model = model.to(device)\n",
        "features = features.to(device)\n",
        "adj = adj.to(device) # Make sure you move the adjacency matrix to the correct device\n",
        "labels = labels.to(device)\n",
        "idx_train = idx_train.to(device)\n",
        "idx_val = idx_val.to(device)\n",
        "idx_test = idx_test.to(device)\n",
        "\n",
        "# And preparing for CUDA operations\n",
        "if torch.cuda.is_available():\n",
        "    model.cuda()\n",
        "    features = features.cuda()\n",
        "    adj_layer_1 = adj_layer_1.cuda()\n",
        "    adj_layer_2 = adj_layer_2.cuda()\n",
        "    labels = labels.cuda()\n",
        "    idx_train.cuda()\n",
        "    idx_val.cuda()\n",
        "    idx_test.cuda()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 167,
      "id": "146eb197",
      "metadata": {
        "id": "146eb197"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "def train(epoch, model, optimizer, features, adj, labels, train_ids):\n",
        "    t = time.time()\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    output = model(features, adj)\n",
        "    loss_train = F.nll_loss(output[train_ids], labels[train_ids])\n",
        "    acc_train = accuracy(output[train_ids], labels[train_ids])\n",
        "    loss_train.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    model.eval()\n",
        "    output = model(features, adj)\n",
        "    loss_val = F.nll_loss(output[idx_val], labels[idx_val])\n",
        "    acc_val = accuracy(output[idx_val], labels[idx_val])\n",
        "\n",
        "    print('Epoch: {:04d}'.format(epoch+1),\n",
        "          'loss_train: {:.4f}'.format(loss_train.item()),\n",
        "          'acc_train: {:.4f}'.format(acc_train.item()),\n",
        "          'loss_val: {:.4f}'.format(loss_val.item()),\n",
        "          'acc_val: {:.4f}'.format(acc_val.item()),\n",
        "          'time: {:.4f}s'.format(time.time() - t))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 168,
      "id": "b32be098",
      "metadata": {
        "id": "b32be098"
      },
      "outputs": [],
      "source": [
        "def test(model, features, adj, labels, idx_test):\n",
        "    model.eval()\n",
        "    output = model(features, adj)\n",
        "    loss_test = F.nll_loss(output[idx_test], labels[idx_test])\n",
        "    acc_test = accuracy(output[idx_test], labels[idx_test])\n",
        "\n",
        "    _, predicted = torch.max(output[idx_test], 1)\n",
        "    precision = precision_score(labels[idx_test].cpu(), predicted.cpu(), average='macro')\n",
        "    recall = recall_score(labels[idx_test].cpu(), predicted.cpu(), average='macro')\n",
        "    f1 = f1_score(labels[idx_test].cpu(), predicted.cpu(), average='macro')\n",
        "\n",
        "    print(\"Test set results:\",\n",
        "          \"loss= {:.4f}\".format(loss_test.item()),\n",
        "          \"accuracy= {:.4f}\".format(acc_test.item()),\n",
        "          \"precision= {:.4f}\".format(precision),\n",
        "          \"recall= {:.4f}\".format(recall),\n",
        "          \"F1= {:.4f}\".format(f1))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train and test the model\n",
        "for epoch in range(200):\n",
        "    train(epoch, model, optimizer, features, adj, labels, idx_train)\n",
        "\n",
        "print()\n",
        "print('Optimization finished...')\n",
        "\n",
        "\n",
        "\n",
        "test(model, features, adj, labels, idx_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WJ-b4KjH0Yvl",
        "outputId": "fbcf6c09-d1c2-49a7-b0ce-4ac3db9c324c"
      },
      "id": "WJ-b4KjH0Yvl",
      "execution_count": 169,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0001 loss_train: 1.9718 acc_train: 0.0786 loss_val: 1.9485 acc_val: 0.0833 time: 0.0086s\n",
            "Epoch: 0002 loss_train: 1.9526 acc_train: 0.0786 loss_val: 1.9333 acc_val: 0.0833 time: 0.0053s\n",
            "Epoch: 0003 loss_train: 1.9343 acc_train: 0.0857 loss_val: 1.9196 acc_val: 0.0867 time: 0.0051s\n",
            "Epoch: 0004 loss_train: 1.9222 acc_train: 0.1143 loss_val: 1.9066 acc_val: 0.0900 time: 0.0052s\n",
            "Epoch: 0005 loss_train: 1.8991 acc_train: 0.1714 loss_val: 1.8943 acc_val: 0.1767 time: 0.0049s\n",
            "Epoch: 0006 loss_train: 1.8890 acc_train: 0.2500 loss_val: 1.8828 acc_val: 0.2967 time: 0.0057s\n",
            "Epoch: 0007 loss_train: 1.8605 acc_train: 0.3286 loss_val: 1.8720 acc_val: 0.3567 time: 0.0049s\n",
            "Epoch: 0008 loss_train: 1.8669 acc_train: 0.2857 loss_val: 1.8617 acc_val: 0.3700 time: 0.0050s\n",
            "Epoch: 0009 loss_train: 1.8323 acc_train: 0.3500 loss_val: 1.8521 acc_val: 0.3800 time: 0.0051s\n",
            "Epoch: 0010 loss_train: 1.8326 acc_train: 0.3571 loss_val: 1.8432 acc_val: 0.3800 time: 0.0050s\n",
            "Epoch: 0011 loss_train: 1.8083 acc_train: 0.3929 loss_val: 1.8349 acc_val: 0.3800 time: 0.0051s\n",
            "Epoch: 0012 loss_train: 1.8124 acc_train: 0.3286 loss_val: 1.8272 acc_val: 0.3833 time: 0.0049s\n",
            "Epoch: 0013 loss_train: 1.7987 acc_train: 0.3857 loss_val: 1.8200 acc_val: 0.3933 time: 0.0050s\n",
            "Epoch: 0014 loss_train: 1.7710 acc_train: 0.4214 loss_val: 1.8134 acc_val: 0.4067 time: 0.0049s\n",
            "Epoch: 0015 loss_train: 1.7486 acc_train: 0.3714 loss_val: 1.8072 acc_val: 0.4133 time: 0.0050s\n",
            "Epoch: 0016 loss_train: 1.7555 acc_train: 0.3857 loss_val: 1.8012 acc_val: 0.4167 time: 0.0070s\n",
            "Epoch: 0017 loss_train: 1.7267 acc_train: 0.3929 loss_val: 1.7951 acc_val: 0.4167 time: 0.0051s\n",
            "Epoch: 0018 loss_train: 1.7070 acc_train: 0.3857 loss_val: 1.7889 acc_val: 0.4167 time: 0.0051s\n",
            "Epoch: 0019 loss_train: 1.7140 acc_train: 0.3786 loss_val: 1.7827 acc_val: 0.4167 time: 0.0049s\n",
            "Epoch: 0020 loss_train: 1.7037 acc_train: 0.3714 loss_val: 1.7758 acc_val: 0.4167 time: 0.0053s\n",
            "Epoch: 0021 loss_train: 1.6799 acc_train: 0.4214 loss_val: 1.7686 acc_val: 0.4167 time: 0.0050s\n",
            "Epoch: 0022 loss_train: 1.6525 acc_train: 0.4000 loss_val: 1.7602 acc_val: 0.4167 time: 0.0050s\n",
            "Epoch: 0023 loss_train: 1.6288 acc_train: 0.4357 loss_val: 1.7512 acc_val: 0.4133 time: 0.0050s\n",
            "Epoch: 0024 loss_train: 1.6150 acc_train: 0.4214 loss_val: 1.7421 acc_val: 0.4033 time: 0.0049s\n",
            "Epoch: 0025 loss_train: 1.6472 acc_train: 0.3857 loss_val: 1.7326 acc_val: 0.4000 time: 0.0049s\n",
            "Epoch: 0026 loss_train: 1.6114 acc_train: 0.4000 loss_val: 1.7228 acc_val: 0.3967 time: 0.0049s\n",
            "Epoch: 0027 loss_train: 1.5844 acc_train: 0.4071 loss_val: 1.7132 acc_val: 0.3967 time: 0.0048s\n",
            "Epoch: 0028 loss_train: 1.5777 acc_train: 0.4357 loss_val: 1.7037 acc_val: 0.3967 time: 0.0051s\n",
            "Epoch: 0029 loss_train: 1.5584 acc_train: 0.4214 loss_val: 1.6942 acc_val: 0.3967 time: 0.0066s\n",
            "Epoch: 0030 loss_train: 1.5459 acc_train: 0.4357 loss_val: 1.6845 acc_val: 0.4033 time: 0.0068s\n",
            "Epoch: 0031 loss_train: 1.5071 acc_train: 0.4357 loss_val: 1.6747 acc_val: 0.4067 time: 0.0050s\n",
            "Epoch: 0032 loss_train: 1.5025 acc_train: 0.4429 loss_val: 1.6649 acc_val: 0.4100 time: 0.0051s\n",
            "Epoch: 0033 loss_train: 1.4894 acc_train: 0.4857 loss_val: 1.6554 acc_val: 0.4167 time: 0.0050s\n",
            "Epoch: 0034 loss_train: 1.4698 acc_train: 0.4714 loss_val: 1.6457 acc_val: 0.4267 time: 0.0048s\n",
            "Epoch: 0035 loss_train: 1.4472 acc_train: 0.4929 loss_val: 1.6355 acc_val: 0.4233 time: 0.0051s\n",
            "Epoch: 0036 loss_train: 1.4293 acc_train: 0.4714 loss_val: 1.6252 acc_val: 0.4200 time: 0.0068s\n",
            "Epoch: 0037 loss_train: 1.4149 acc_train: 0.4929 loss_val: 1.6143 acc_val: 0.4267 time: 0.0049s\n",
            "Epoch: 0038 loss_train: 1.3819 acc_train: 0.5214 loss_val: 1.6026 acc_val: 0.4300 time: 0.0049s\n",
            "Epoch: 0039 loss_train: 1.3560 acc_train: 0.5571 loss_val: 1.5907 acc_val: 0.4333 time: 0.0049s\n",
            "Epoch: 0040 loss_train: 1.3326 acc_train: 0.5357 loss_val: 1.5786 acc_val: 0.4400 time: 0.0051s\n",
            "Epoch: 0041 loss_train: 1.3055 acc_train: 0.5500 loss_val: 1.5664 acc_val: 0.4333 time: 0.0049s\n",
            "Epoch: 0042 loss_train: 1.2958 acc_train: 0.5429 loss_val: 1.5538 acc_val: 0.4367 time: 0.0050s\n",
            "Epoch: 0043 loss_train: 1.2657 acc_train: 0.5929 loss_val: 1.5408 acc_val: 0.4433 time: 0.0050s\n",
            "Epoch: 0044 loss_train: 1.2163 acc_train: 0.6000 loss_val: 1.5278 acc_val: 0.4467 time: 0.0049s\n",
            "Epoch: 0045 loss_train: 1.2241 acc_train: 0.5857 loss_val: 1.5142 acc_val: 0.4633 time: 0.0049s\n",
            "Epoch: 0046 loss_train: 1.1952 acc_train: 0.6286 loss_val: 1.4998 acc_val: 0.4733 time: 0.0079s\n",
            "Epoch: 0047 loss_train: 1.1998 acc_train: 0.6071 loss_val: 1.4860 acc_val: 0.4767 time: 0.0050s\n",
            "Epoch: 0048 loss_train: 1.1391 acc_train: 0.6714 loss_val: 1.4722 acc_val: 0.5033 time: 0.0049s\n",
            "Epoch: 0049 loss_train: 1.0845 acc_train: 0.7143 loss_val: 1.4588 acc_val: 0.5100 time: 0.0051s\n",
            "Epoch: 0050 loss_train: 1.0903 acc_train: 0.6714 loss_val: 1.4451 acc_val: 0.5100 time: 0.0050s\n",
            "Epoch: 0051 loss_train: 1.0568 acc_train: 0.7000 loss_val: 1.4312 acc_val: 0.5267 time: 0.0049s\n",
            "Epoch: 0052 loss_train: 1.0535 acc_train: 0.6643 loss_val: 1.4178 acc_val: 0.5400 time: 0.0051s\n",
            "Epoch: 0053 loss_train: 0.9952 acc_train: 0.7857 loss_val: 1.4044 acc_val: 0.5567 time: 0.0049s\n",
            "Epoch: 0054 loss_train: 1.0018 acc_train: 0.7571 loss_val: 1.3915 acc_val: 0.5567 time: 0.0048s\n",
            "Epoch: 0055 loss_train: 0.9920 acc_train: 0.7786 loss_val: 1.3782 acc_val: 0.5633 time: 0.0047s\n",
            "Epoch: 0056 loss_train: 0.9600 acc_train: 0.7786 loss_val: 1.3656 acc_val: 0.5767 time: 0.0050s\n",
            "Epoch: 0057 loss_train: 0.9500 acc_train: 0.7714 loss_val: 1.3532 acc_val: 0.5833 time: 0.0051s\n",
            "Epoch: 0058 loss_train: 0.9077 acc_train: 0.7643 loss_val: 1.3410 acc_val: 0.6000 time: 0.0086s\n",
            "Epoch: 0059 loss_train: 0.8731 acc_train: 0.7857 loss_val: 1.3287 acc_val: 0.6167 time: 0.0049s\n",
            "Epoch: 0060 loss_train: 0.8569 acc_train: 0.8071 loss_val: 1.3177 acc_val: 0.6200 time: 0.0058s\n",
            "Epoch: 0061 loss_train: 0.8405 acc_train: 0.8000 loss_val: 1.3073 acc_val: 0.6300 time: 0.0052s\n",
            "Epoch: 0062 loss_train: 0.8664 acc_train: 0.8000 loss_val: 1.2970 acc_val: 0.6467 time: 0.0051s\n",
            "Epoch: 0063 loss_train: 0.8434 acc_train: 0.8357 loss_val: 1.2877 acc_val: 0.6500 time: 0.0050s\n",
            "Epoch: 0064 loss_train: 0.8153 acc_train: 0.8071 loss_val: 1.2792 acc_val: 0.6633 time: 0.0051s\n",
            "Epoch: 0065 loss_train: 0.7588 acc_train: 0.8500 loss_val: 1.2706 acc_val: 0.6633 time: 0.0081s\n",
            "Epoch: 0066 loss_train: 0.7355 acc_train: 0.8429 loss_val: 1.2607 acc_val: 0.6633 time: 0.0071s\n",
            "Epoch: 0067 loss_train: 0.7434 acc_train: 0.8286 loss_val: 1.2506 acc_val: 0.6600 time: 0.0051s\n",
            "Epoch: 0068 loss_train: 0.7498 acc_train: 0.8571 loss_val: 1.2416 acc_val: 0.6633 time: 0.0050s\n",
            "Epoch: 0069 loss_train: 0.7321 acc_train: 0.8571 loss_val: 1.2328 acc_val: 0.6667 time: 0.0052s\n",
            "Epoch: 0070 loss_train: 0.7069 acc_train: 0.8500 loss_val: 1.2248 acc_val: 0.6733 time: 0.0071s\n",
            "Epoch: 0071 loss_train: 0.7144 acc_train: 0.8429 loss_val: 1.2176 acc_val: 0.6767 time: 0.0051s\n",
            "Epoch: 0072 loss_train: 0.7029 acc_train: 0.8429 loss_val: 1.2116 acc_val: 0.6900 time: 0.0049s\n",
            "Epoch: 0073 loss_train: 0.6622 acc_train: 0.8929 loss_val: 1.2053 acc_val: 0.6933 time: 0.0051s\n",
            "Epoch: 0074 loss_train: 0.6507 acc_train: 0.8786 loss_val: 1.1991 acc_val: 0.6933 time: 0.0050s\n",
            "Epoch: 0075 loss_train: 0.6330 acc_train: 0.8786 loss_val: 1.1934 acc_val: 0.7000 time: 0.0049s\n",
            "Epoch: 0076 loss_train: 0.6421 acc_train: 0.8643 loss_val: 1.1877 acc_val: 0.7033 time: 0.0070s\n",
            "Epoch: 0077 loss_train: 0.6316 acc_train: 0.8786 loss_val: 1.1826 acc_val: 0.7100 time: 0.0051s\n",
            "Epoch: 0078 loss_train: 0.6548 acc_train: 0.8571 loss_val: 1.1764 acc_val: 0.7133 time: 0.0050s\n",
            "Epoch: 0079 loss_train: 0.6183 acc_train: 0.8857 loss_val: 1.1702 acc_val: 0.7000 time: 0.0049s\n",
            "Epoch: 0080 loss_train: 0.5797 acc_train: 0.8929 loss_val: 1.1633 acc_val: 0.6967 time: 0.0050s\n",
            "Epoch: 0081 loss_train: 0.6022 acc_train: 0.8714 loss_val: 1.1574 acc_val: 0.6967 time: 0.0050s\n",
            "Epoch: 0082 loss_train: 0.5836 acc_train: 0.9214 loss_val: 1.1523 acc_val: 0.6967 time: 0.0065s\n",
            "Epoch: 0083 loss_train: 0.5921 acc_train: 0.8500 loss_val: 1.1474 acc_val: 0.7000 time: 0.0073s\n",
            "Epoch: 0084 loss_train: 0.6088 acc_train: 0.8786 loss_val: 1.1429 acc_val: 0.6967 time: 0.0049s\n",
            "Epoch: 0085 loss_train: 0.5308 acc_train: 0.8929 loss_val: 1.1384 acc_val: 0.6967 time: 0.0087s\n",
            "Epoch: 0086 loss_train: 0.5463 acc_train: 0.8786 loss_val: 1.1341 acc_val: 0.6967 time: 0.0049s\n",
            "Epoch: 0087 loss_train: 0.5717 acc_train: 0.9357 loss_val: 1.1306 acc_val: 0.6967 time: 0.0051s\n",
            "Epoch: 0088 loss_train: 0.5270 acc_train: 0.8929 loss_val: 1.1267 acc_val: 0.6900 time: 0.0049s\n",
            "Epoch: 0089 loss_train: 0.5114 acc_train: 0.9071 loss_val: 1.1228 acc_val: 0.6933 time: 0.0051s\n",
            "Epoch: 0090 loss_train: 0.5695 acc_train: 0.8857 loss_val: 1.1184 acc_val: 0.6900 time: 0.0052s\n",
            "Epoch: 0091 loss_train: 0.5277 acc_train: 0.9000 loss_val: 1.1149 acc_val: 0.6933 time: 0.0051s\n",
            "Epoch: 0092 loss_train: 0.5295 acc_train: 0.9000 loss_val: 1.1114 acc_val: 0.6967 time: 0.0051s\n",
            "Epoch: 0093 loss_train: 0.4753 acc_train: 0.9286 loss_val: 1.1083 acc_val: 0.6900 time: 0.0051s\n",
            "Epoch: 0094 loss_train: 0.4852 acc_train: 0.9000 loss_val: 1.1053 acc_val: 0.6900 time: 0.0050s\n",
            "Epoch: 0095 loss_train: 0.5449 acc_train: 0.8929 loss_val: 1.1025 acc_val: 0.6967 time: 0.0051s\n",
            "Epoch: 0096 loss_train: 0.5829 acc_train: 0.8786 loss_val: 1.1005 acc_val: 0.6967 time: 0.0051s\n",
            "Epoch: 0097 loss_train: 0.4266 acc_train: 0.9429 loss_val: 1.0984 acc_val: 0.7033 time: 0.0050s\n",
            "Epoch: 0098 loss_train: 0.5327 acc_train: 0.8929 loss_val: 1.0959 acc_val: 0.7000 time: 0.0050s\n",
            "Epoch: 0099 loss_train: 0.5110 acc_train: 0.8929 loss_val: 1.0941 acc_val: 0.7000 time: 0.0051s\n",
            "Epoch: 0100 loss_train: 0.4460 acc_train: 0.9143 loss_val: 1.0926 acc_val: 0.7033 time: 0.0055s\n",
            "Epoch: 0101 loss_train: 0.5056 acc_train: 0.8929 loss_val: 1.0902 acc_val: 0.7033 time: 0.0051s\n",
            "Epoch: 0102 loss_train: 0.4572 acc_train: 0.9357 loss_val: 1.0873 acc_val: 0.7067 time: 0.0050s\n",
            "Epoch: 0103 loss_train: 0.4450 acc_train: 0.9429 loss_val: 1.0843 acc_val: 0.7067 time: 0.0050s\n",
            "Epoch: 0104 loss_train: 0.4982 acc_train: 0.8929 loss_val: 1.0822 acc_val: 0.7067 time: 0.0051s\n",
            "Epoch: 0105 loss_train: 0.4352 acc_train: 0.9214 loss_val: 1.0805 acc_val: 0.7000 time: 0.0056s\n",
            "Epoch: 0106 loss_train: 0.4288 acc_train: 0.9286 loss_val: 1.0795 acc_val: 0.7033 time: 0.0071s\n",
            "Epoch: 0107 loss_train: 0.4477 acc_train: 0.9286 loss_val: 1.0780 acc_val: 0.7000 time: 0.0050s\n",
            "Epoch: 0108 loss_train: 0.4711 acc_train: 0.8857 loss_val: 1.0775 acc_val: 0.7000 time: 0.0050s\n",
            "Epoch: 0109 loss_train: 0.4544 acc_train: 0.9214 loss_val: 1.0775 acc_val: 0.6967 time: 0.0055s\n",
            "Epoch: 0110 loss_train: 0.4150 acc_train: 0.9357 loss_val: 1.0764 acc_val: 0.6967 time: 0.0114s\n",
            "Epoch: 0111 loss_train: 0.4116 acc_train: 0.9286 loss_val: 1.0745 acc_val: 0.7033 time: 0.0057s\n",
            "Epoch: 0112 loss_train: 0.4414 acc_train: 0.9143 loss_val: 1.0727 acc_val: 0.7033 time: 0.0128s\n",
            "Epoch: 0113 loss_train: 0.3761 acc_train: 0.9429 loss_val: 1.0708 acc_val: 0.7067 time: 0.0052s\n",
            "Epoch: 0114 loss_train: 0.4055 acc_train: 0.9214 loss_val: 1.0693 acc_val: 0.7033 time: 0.0049s\n",
            "Epoch: 0115 loss_train: 0.3875 acc_train: 0.9357 loss_val: 1.0677 acc_val: 0.7033 time: 0.0050s\n",
            "Epoch: 0116 loss_train: 0.3760 acc_train: 0.9357 loss_val: 1.0665 acc_val: 0.6967 time: 0.0053s\n",
            "Epoch: 0117 loss_train: 0.4025 acc_train: 0.9071 loss_val: 1.0653 acc_val: 0.6900 time: 0.0052s\n",
            "Epoch: 0118 loss_train: 0.4217 acc_train: 0.8929 loss_val: 1.0638 acc_val: 0.6967 time: 0.0050s\n",
            "Epoch: 0119 loss_train: 0.4456 acc_train: 0.9214 loss_val: 1.0621 acc_val: 0.6967 time: 0.0051s\n",
            "Epoch: 0120 loss_train: 0.3648 acc_train: 0.9357 loss_val: 1.0601 acc_val: 0.7000 time: 0.0049s\n",
            "Epoch: 0121 loss_train: 0.4187 acc_train: 0.9214 loss_val: 1.0580 acc_val: 0.7067 time: 0.0050s\n",
            "Epoch: 0122 loss_train: 0.3726 acc_train: 0.9143 loss_val: 1.0564 acc_val: 0.7000 time: 0.0050s\n",
            "Epoch: 0123 loss_train: 0.3943 acc_train: 0.9143 loss_val: 1.0552 acc_val: 0.6967 time: 0.0050s\n",
            "Epoch: 0124 loss_train: 0.4057 acc_train: 0.9500 loss_val: 1.0542 acc_val: 0.7000 time: 0.0067s\n",
            "Epoch: 0125 loss_train: 0.3726 acc_train: 0.9429 loss_val: 1.0534 acc_val: 0.7033 time: 0.0089s\n",
            "Epoch: 0126 loss_train: 0.3390 acc_train: 0.9714 loss_val: 1.0518 acc_val: 0.7033 time: 0.0060s\n",
            "Epoch: 0127 loss_train: 0.4024 acc_train: 0.9429 loss_val: 1.0500 acc_val: 0.7000 time: 0.0053s\n",
            "Epoch: 0128 loss_train: 0.4198 acc_train: 0.9071 loss_val: 1.0484 acc_val: 0.6967 time: 0.0053s\n",
            "Epoch: 0129 loss_train: 0.4169 acc_train: 0.9214 loss_val: 1.0478 acc_val: 0.7033 time: 0.0057s\n",
            "Epoch: 0130 loss_train: 0.3680 acc_train: 0.9286 loss_val: 1.0482 acc_val: 0.7000 time: 0.0050s\n",
            "Epoch: 0131 loss_train: 0.3356 acc_train: 0.9714 loss_val: 1.0474 acc_val: 0.7033 time: 0.0050s\n",
            "Epoch: 0132 loss_train: 0.3654 acc_train: 0.9643 loss_val: 1.0454 acc_val: 0.7033 time: 0.0051s\n",
            "Epoch: 0133 loss_train: 0.4079 acc_train: 0.8857 loss_val: 1.0435 acc_val: 0.7067 time: 0.0050s\n",
            "Epoch: 0134 loss_train: 0.3765 acc_train: 0.9500 loss_val: 1.0418 acc_val: 0.7100 time: 0.0050s\n",
            "Epoch: 0135 loss_train: 0.3827 acc_train: 0.9071 loss_val: 1.0411 acc_val: 0.7000 time: 0.0052s\n",
            "Epoch: 0136 loss_train: 0.4112 acc_train: 0.9357 loss_val: 1.0401 acc_val: 0.6967 time: 0.0053s\n",
            "Epoch: 0137 loss_train: 0.3818 acc_train: 0.9214 loss_val: 1.0387 acc_val: 0.7000 time: 0.0054s\n",
            "Epoch: 0138 loss_train: 0.3589 acc_train: 0.9357 loss_val: 1.0371 acc_val: 0.7067 time: 0.0051s\n",
            "Epoch: 0139 loss_train: 0.3781 acc_train: 0.9429 loss_val: 1.0351 acc_val: 0.7067 time: 0.0098s\n",
            "Epoch: 0140 loss_train: 0.3703 acc_train: 0.9143 loss_val: 1.0342 acc_val: 0.7067 time: 0.0082s\n",
            "Epoch: 0141 loss_train: 0.3629 acc_train: 0.9286 loss_val: 1.0327 acc_val: 0.7033 time: 0.0051s\n",
            "Epoch: 0142 loss_train: 0.3394 acc_train: 0.9357 loss_val: 1.0313 acc_val: 0.7067 time: 0.0067s\n",
            "Epoch: 0143 loss_train: 0.3507 acc_train: 0.9500 loss_val: 1.0307 acc_val: 0.7033 time: 0.0051s\n",
            "Epoch: 0144 loss_train: 0.3303 acc_train: 0.9571 loss_val: 1.0302 acc_val: 0.7033 time: 0.0059s\n",
            "Epoch: 0145 loss_train: 0.3508 acc_train: 0.9357 loss_val: 1.0297 acc_val: 0.7033 time: 0.0070s\n",
            "Epoch: 0146 loss_train: 0.3366 acc_train: 0.9357 loss_val: 1.0296 acc_val: 0.7067 time: 0.0051s\n",
            "Epoch: 0147 loss_train: 0.3176 acc_train: 0.9500 loss_val: 1.0290 acc_val: 0.7067 time: 0.0052s\n",
            "Epoch: 0148 loss_train: 0.3437 acc_train: 0.9429 loss_val: 1.0275 acc_val: 0.7000 time: 0.0050s\n",
            "Epoch: 0149 loss_train: 0.3474 acc_train: 0.9286 loss_val: 1.0260 acc_val: 0.6967 time: 0.0050s\n",
            "Epoch: 0150 loss_train: 0.3491 acc_train: 0.9429 loss_val: 1.0249 acc_val: 0.7000 time: 0.0051s\n",
            "Epoch: 0151 loss_train: 0.3421 acc_train: 0.9357 loss_val: 1.0246 acc_val: 0.7000 time: 0.0051s\n",
            "Epoch: 0152 loss_train: 0.3529 acc_train: 0.9286 loss_val: 1.0245 acc_val: 0.6967 time: 0.0051s\n",
            "Epoch: 0153 loss_train: 0.3785 acc_train: 0.9000 loss_val: 1.0245 acc_val: 0.7000 time: 0.0051s\n",
            "Epoch: 0154 loss_train: 0.3732 acc_train: 0.9357 loss_val: 1.0244 acc_val: 0.7000 time: 0.0051s\n",
            "Epoch: 0155 loss_train: 0.4053 acc_train: 0.9429 loss_val: 1.0236 acc_val: 0.6900 time: 0.0051s\n",
            "Epoch: 0156 loss_train: 0.3365 acc_train: 0.9214 loss_val: 1.0228 acc_val: 0.6900 time: 0.0052s\n",
            "Epoch: 0157 loss_train: 0.3278 acc_train: 0.9286 loss_val: 1.0215 acc_val: 0.6933 time: 0.0051s\n",
            "Epoch: 0158 loss_train: 0.3933 acc_train: 0.9357 loss_val: 1.0201 acc_val: 0.6967 time: 0.0050s\n",
            "Epoch: 0159 loss_train: 0.3631 acc_train: 0.9143 loss_val: 1.0192 acc_val: 0.6933 time: 0.0050s\n",
            "Epoch: 0160 loss_train: 0.3354 acc_train: 0.9143 loss_val: 1.0189 acc_val: 0.6900 time: 0.0050s\n",
            "Epoch: 0161 loss_train: 0.3927 acc_train: 0.9214 loss_val: 1.0184 acc_val: 0.6933 time: 0.0049s\n",
            "Epoch: 0162 loss_train: 0.3479 acc_train: 0.9286 loss_val: 1.0189 acc_val: 0.6967 time: 0.0049s\n",
            "Epoch: 0163 loss_train: 0.3325 acc_train: 0.9357 loss_val: 1.0195 acc_val: 0.7000 time: 0.0048s\n",
            "Epoch: 0164 loss_train: 0.3072 acc_train: 0.9357 loss_val: 1.0204 acc_val: 0.7000 time: 0.0050s\n",
            "Epoch: 0165 loss_train: 0.3485 acc_train: 0.9500 loss_val: 1.0204 acc_val: 0.7000 time: 0.0050s\n",
            "Epoch: 0166 loss_train: 0.3464 acc_train: 0.9357 loss_val: 1.0201 acc_val: 0.7000 time: 0.0052s\n",
            "Epoch: 0167 loss_train: 0.3414 acc_train: 0.9071 loss_val: 1.0189 acc_val: 0.6967 time: 0.0063s\n",
            "Epoch: 0168 loss_train: 0.3996 acc_train: 0.9071 loss_val: 1.0177 acc_val: 0.6933 time: 0.0079s\n",
            "Epoch: 0169 loss_train: 0.3043 acc_train: 0.9500 loss_val: 1.0161 acc_val: 0.6967 time: 0.0051s\n",
            "Epoch: 0170 loss_train: 0.3695 acc_train: 0.9071 loss_val: 1.0145 acc_val: 0.6967 time: 0.0049s\n",
            "Epoch: 0171 loss_train: 0.3458 acc_train: 0.9357 loss_val: 1.0137 acc_val: 0.7067 time: 0.0049s\n",
            "Epoch: 0172 loss_train: 0.2820 acc_train: 0.9571 loss_val: 1.0130 acc_val: 0.7067 time: 0.0050s\n",
            "Epoch: 0173 loss_train: 0.2883 acc_train: 0.9786 loss_val: 1.0127 acc_val: 0.7133 time: 0.0049s\n",
            "Epoch: 0174 loss_train: 0.3377 acc_train: 0.9643 loss_val: 1.0122 acc_val: 0.7100 time: 0.0048s\n",
            "Epoch: 0175 loss_train: 0.3351 acc_train: 0.9357 loss_val: 1.0125 acc_val: 0.7033 time: 0.0048s\n",
            "Epoch: 0176 loss_train: 0.3574 acc_train: 0.9214 loss_val: 1.0129 acc_val: 0.7000 time: 0.0050s\n",
            "Epoch: 0177 loss_train: 0.3169 acc_train: 0.9429 loss_val: 1.0127 acc_val: 0.6967 time: 0.0050s\n",
            "Epoch: 0178 loss_train: 0.3481 acc_train: 0.9214 loss_val: 1.0115 acc_val: 0.6900 time: 0.0050s\n",
            "Epoch: 0179 loss_train: 0.3079 acc_train: 0.9929 loss_val: 1.0112 acc_val: 0.6933 time: 0.0065s\n",
            "Epoch: 0180 loss_train: 0.3719 acc_train: 0.9071 loss_val: 1.0097 acc_val: 0.6900 time: 0.0052s\n",
            "Epoch: 0181 loss_train: 0.3074 acc_train: 0.9357 loss_val: 1.0086 acc_val: 0.6967 time: 0.0050s\n",
            "Epoch: 0182 loss_train: 0.3139 acc_train: 0.9571 loss_val: 1.0083 acc_val: 0.7033 time: 0.0050s\n",
            "Epoch: 0183 loss_train: 0.3214 acc_train: 0.9357 loss_val: 1.0089 acc_val: 0.7067 time: 0.0051s\n",
            "Epoch: 0184 loss_train: 0.3178 acc_train: 0.9500 loss_val: 1.0098 acc_val: 0.7067 time: 0.0049s\n",
            "Epoch: 0185 loss_train: 0.3088 acc_train: 0.9286 loss_val: 1.0104 acc_val: 0.7033 time: 0.0050s\n",
            "Epoch: 0186 loss_train: 0.3189 acc_train: 0.9429 loss_val: 1.0112 acc_val: 0.7033 time: 0.0051s\n",
            "Epoch: 0187 loss_train: 0.3359 acc_train: 0.9429 loss_val: 1.0119 acc_val: 0.7000 time: 0.0050s\n",
            "Epoch: 0188 loss_train: 0.3642 acc_train: 0.9000 loss_val: 1.0120 acc_val: 0.7033 time: 0.0049s\n",
            "Epoch: 0189 loss_train: 0.3012 acc_train: 0.9714 loss_val: 1.0119 acc_val: 0.7033 time: 0.0049s\n",
            "Epoch: 0190 loss_train: 0.3122 acc_train: 0.9571 loss_val: 1.0115 acc_val: 0.6967 time: 0.0052s\n",
            "Epoch: 0191 loss_train: 0.3311 acc_train: 0.9500 loss_val: 1.0106 acc_val: 0.6967 time: 0.0051s\n",
            "Epoch: 0192 loss_train: 0.2836 acc_train: 0.9643 loss_val: 1.0095 acc_val: 0.6967 time: 0.0051s\n",
            "Epoch: 0193 loss_train: 0.3121 acc_train: 0.9143 loss_val: 1.0094 acc_val: 0.6933 time: 0.0052s\n",
            "Epoch: 0194 loss_train: 0.3472 acc_train: 0.9286 loss_val: 1.0097 acc_val: 0.6967 time: 0.0051s\n",
            "Epoch: 0195 loss_train: 0.3305 acc_train: 0.9071 loss_val: 1.0096 acc_val: 0.7000 time: 0.0050s\n",
            "Epoch: 0196 loss_train: 0.3722 acc_train: 0.8857 loss_val: 1.0096 acc_val: 0.6967 time: 0.0081s\n",
            "Epoch: 0197 loss_train: 0.2989 acc_train: 0.9214 loss_val: 1.0099 acc_val: 0.6967 time: 0.0071s\n",
            "Epoch: 0198 loss_train: 0.3114 acc_train: 0.9429 loss_val: 1.0085 acc_val: 0.6967 time: 0.0050s\n",
            "Epoch: 0199 loss_train: 0.3190 acc_train: 0.9500 loss_val: 1.0073 acc_val: 0.6967 time: 0.0055s\n",
            "Epoch: 0200 loss_train: 0.3062 acc_train: 0.9429 loss_val: 1.0065 acc_val: 0.6967 time: 0.0049s\n",
            "\n",
            "Optimization finished...\n",
            "Test set results: loss= 1.0581 accuracy= 0.6600 precision= 0.6630 recall= 0.6055 F1= 0.6242\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1af85139",
      "metadata": {
        "id": "1af85139"
      },
      "source": [
        "###"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}